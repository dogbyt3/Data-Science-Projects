import numpy as np
import matplotlib.pyplot as plt
from ClassifierLib import *

D = 1  # number of dimensions
N = 10 # sample number

X1 = np.random.normal(1.0,0.1,(N,D))
T1 = np.array([1]*N).reshape((N, 1))
X2 = np.random.normal(2.0,0.1,(N,D))
T2 = np.array([2]*N).reshape((N, 1))
X3 = np.random.normal(3.0,0.1,(N,D))
T3 = np.array([3]*N).reshape((N, 1))

data = np.hstack((np.vstack((X1,X2,X3)), np.vstack((T1,T2,T3))))

X = data[:,0:D]
T = data[:,-1]
standardize,unstandardize = makeStandardizeF(X)
Xs = standardize(X)

class1rows = T==1
class2rows = T==2
class3rows = T==3

mu1 = np.mean(Xs[class1rows,:],axis=0)
mu2 = np.mean(Xs[class2rows,:],axis=0)
mu3 = np.mean(Xs[class3rows,:],axis=0)

sigma1 = np.cov(Xs[class1rows,:].T)
sigma2 = np.cov(Xs[class2rows,:].T)
sigma3 = np.cov(Xs[class3rows,:].T)

N1 = np.sum(class1rows)
N2 = np.sum(class2rows)
N3 = np.sum(class3rows)

prior1 = N1/float(N)
prior2 = N2/float(N)
prior3 = N3/float(N)

# generate some new data
nNew = 100
newData = np.linspace(0.,4.0,nNew).repeat(D).reshape((nNew,D))

# apply QDA discr to training data
d1 = discQDA(X,standardize,mu1,sigma1,prior1)
d2 = discQDA(X,standardize,mu2,sigma2,prior2)
d3 = discQDA(X,standardize,mu3,sigma3,prior3)
QDApredictedTrain = np.argmax(np.vstack((d1,d2,d3)),axis=0)

# apply QDA discr to testing data
d1t = discQDA(newData,standardize,mu1,sigma1,prior1)
d2t = discQDA(newData,standardize,mu2,sigma2,prior2)
d3t = discQDA(newData,standardize,mu3,sigma3,prior3)
QDApredictedTest = np.argmax(np.vstack((d1t,d2t,d3t)),axis=0)

# apply LDA discr to training data
ld1 = discQDA(X,standardize,mu1,sigma1,prior1)
ld2 = discQDA(X,standardize,mu2,sigma2,prior2)
ld3 = discQDA(X,standardize,mu3,sigma3,prior3)
LDApredictedTrain = np.argmax(np.vstack((ld1,ld2,ld3)),axis=0)

# apply LDA discr to testing data
ld1t = discLDA(newData,standardize,mu1,sigma1,prior1)
ld2t = discLDA(newData,standardize,mu2,sigma2,prior2)
ld3t = discLDA(newData,standardize,mu3,sigma3,prior3)
LDApredictedTest = np.argmax(np.vstack((ld1t,ld2t,ld3t)),axis=0)

#print"QDA Test Prediction %",percentCorrect(QDApredictedTest, T), "LDA Test Prediction %",percentCorrect(LDApredictedTest,T)

###############################################################################
########## QDA Plots
###############################################################################

############# QDA Plot 1
#
# the training data, as its x value versus the class (1, 2 or 3)
#
plt.figure(1)
plt.clf()
plt.subplot(6,1,1)
makePlot1(X1, X2, X3, T1, T2, T3)

############# QDA Plot 2
#
# the three curves for p(x|Class=k) for k=1,2,3, for x values in a set of test 
# data generated by x = np.linspace(0,4,100), where p(x|C=k) is calculated 
# using means and standard deviations for each class calculated from the 
# training data
#

probs = np.exp(np.vstack((d1t-np.log(prior1),d2t-np.log(prior2),\
                          d3t-np.log(prior3))).T \
                          - 0.5*D*np.log(2*np.pi))
print("probs.shape: "+str(probs.shape))

makePlot2(newData, probs, "QDA")

############# QDA Plot 3
#
# the curve for p(x) for the test data
# p(x)=sum(k=1,K)[(p(x|C=k)*p(C=k))] = sum(k=1,K)[p(x|C=k)*1/3]
probs2 = np.array(((probs)/3.0),dtype=np.float32)
px = np.sum(probs2,axis=1)[:,np.newaxis]
makePlot3(newData, px, "QDA")

############# QDA Plot 4
#
# the three curves for p(C=k|x) for k = 1, 2, and 3, for the test data
# p(C=k|x) = (p(x|C=k) * p(C=k))/p(x) = ((probs)(1/3))/(probs2)
#
probs3 = np.array(((probs/3.0)/px),dtype=np.float32)
makePlot4(newData, probs3, "QDA")

############# QDA Plot 5
#
# the three discriminant functions for the test data
#
makePlot5(newData, d1t, d2t, d3t, "QDA")

############# QDA Plot 6
#
# the class predicted by the classifier for the test data
#
makePlot6(newData, d1t, d2t, d3t, "QDA")
plt.show()

###############################################################################
########## LDA Plots
###############################################################################

############# LDA Plot 1
#
# the training data, as its x value versus the class (1, 2 or 3)
#
plt.figure(1)
plt.clf()
plt.subplot(6,1,1)
makePlot1(X1, X2, X3, T1, T2, T3)

############# LDA Plot 2
#
# the three curves for p(x|Class=k) for k=1,2,3, for x values in a set of test 
# data generated by x = np.linspace(0,4,100), where p(x|C=k) is calculated 
# using means and standard deviations for each class calculated from the 
# training data
#
prob1 = normald(standardize(newData), mu1, sigma1)
prob2 = normald(standardize(newData), mu2, sigma2)
prob3 = normald(standardize(newData), mu3, sigma3)
probs = np.hstack((prob1,prob2,prob3))
makePlot2(newData, probs, "LDA")

############# LDA Plot 3
#
# the curve for p(x) for the test data
# p(x)=sum(k=1,K)[(p(x|C=k)*p(C=k))] = sum(k=1,K)[p(x|C=k)*1/3]
#
probs2 = np.array(((probs)/3.0),dtype=np.float32)
px = np.sum(probs2,axis=1)[:,np.newaxis]
makePlot3(newData, px, "LDA")

############# LDA Plot 4
#
# the three curves for p(C=k|x) for k = 1, 2, and 3, for the test data
# p(C=k|x) = (p(x|C=k) * p(C=k))/p(x) = (probs1 * 1/3 ) / probs2 
#
probs3 = np.array(((probs/3.0)/px),dtype=np.float32)
makePlot4(newData, probs3, "LDA")

############# LDA Plot 5
#
# the three discriminant functions for the test data
#
makePlot5(newData, ld1t, ld2t, ld3t, "LDA")

############# LDA Plot 6
#
# the class predicted by the classifier for the test data
#
makePlot6(newData, ld1t, ld2t, ld3t, "LDA")
plt.show()

###############################################################################
########## Logistic Regression Plots
###############################################################################

### Run LR for 1000 iterations
iter = 1000

TtrainI = makeIndicatorVars(T)
Xtrain = addOnes(Xs)

beta = np.zeros((Xtrain.shape[1],TtrainI.shape[1]-1))
alpha = 0.0001

for step in range(iter):
    gs = g(Xtrain,beta)
    beta = beta + alpha * np.dot(Xtrain.T, TtrainI[:,:-1] - gs[:,:-1])

logregOutput = g(Xtrain, beta)
predictedTrain = np.argmax(logregOutput, axis=1)
LRpredictedTrain = (predictedTrain+1)[:,np.newaxis]

Xtest = addOnes(standardize(newData))

logregOutput = g(Xtest, beta)
predictedTest = np.argmax(logregOutput, axis=1)
LRpredictedTest = (predictedTest+1)[:,np.newaxis]

############# LR Plot 1
#
# the training data, as its x value versus the class (1, 2 or 3)
#
plt.figure(1)
plt.clf()
plt.subplot(4,1,1)
makePlot1(X1, X2, X3, T1, T2, T3)

############# LR Plot 2
#
# the three curves for p(x|Class=k) for k=1,2,3, for x values in a set of test 
# data generated by x = np.linspace(0,4,100), where p(x|C=k) is calculated 
# using means and standard deviations for each class calculated from the 
# training data
#
makeLRPlot2(newData, prob1, prob2, prob3, "LR")

############# LR Plot 3
#
# the class predicted by the classifier for the test data
#
makeLRPlot3(newData, predictedTest, "LR")

############# LR Plot 4
#
# plot the g's from LR
#
makeLRPlot4(newData, logregOutput, "LR")
plt.show()

Ttrain = T
Ttest = T

######## print out the predicted percentages of the three methods
print "QDA Percent correct: Train[",Ttrain.shape,"]",percentCorrect(QDApredictedTrain,Ttrain),"Test[",Ttest.shape,"]",percentCorrect(QDApredictedTest,Ttest)
print "LDA Percent correct: Train[",Ttrain.shape,"]",percentCorrect(LDApredictedTrain,Ttrain),"Test[",Ttest.shape,"]",percentCorrect(LDApredictedTest,Ttest)
print "LR Percent correct: Train[",Ttrain.shape,"]",percentCorrect(LRpredictedTrain,Ttrain),"Test[",Ttest.shape,"]",percentCorrect(LRpredictedTest,Ttest)

